title: "Github Ecological Notebook"

output: html_document
editor_options: 
chunk_output_type: console
---

## Author:  Bertrand Black
### Affiliation: University of Vermont
### E-mail contact: bblack@uvm.edu


### Start Date: 2020-01-13
### End Date: 2020-05-08
### Project Descriptions:   





# Table of Contents:  
  * [Entry 1: 2020-01-13, Monday](#id-section1)
      * [Entry 2: 2020-01-14, Tuesday](#id-section2)
    * [Entry 3: 2020-01-15, Wednesday](#id-section3)
     * [Entry 4: 2020-01-16, Thursday](#id-section4)
      * [Entry 5: 2020-01-17, Friday](#id-section5)
      * [Entry 6: 2020-01-20, Monday](#id-section6)
      * [Entry 7: 2020-01-21, Tuesday](#id-section7)
    * [Entry 8: 2020-01-22, Wednesday](#id-section8)
     * [Entry 9: 2020-01-23, Thursday](#id-section9)
    * [Entry 10: 2020-01-24, Friday](#id-section10)
    * [Entry 11: 2020-01-27, Monday](#id-section11)
     * [Entry 12: 2020-01-28, Tuesday](#id-section12)
     * [Entry 13: 2020-01-29, Wednesday](#id-section13)
     * [Entry 14: 2020-01-30, Thursday](#id-section14)
    * [Entry 15: 2020-01-31, Friday](#id-section15)
    * [Entry 16: 2020-02-03, Monday](#id-section16)
     * [Entry 17: 2020-02-04, Tuesday](#id-section17)
    * [Entry 18: 2020-02-05, Wednesday](#id-section18)
    * [Entry 19: 2020-02-06, Thursday](#id-section19)
    * [Entry 20: 2020-02-07, Friday](#id-section20)
    * [Entry 21: 2020-02-10, Monday](#id-section21)
    * [Entry 22: 2020-02-11, Tuesday](#id-section22)
     * [Entry 23: 2020-02-12, Wednesday](#id-section23)
    * [Entry 24: 2020-02-13, Thursday](#id-section24)
    * [Entry 25: 2020-02-14, Friday](#id-section25)
    * [Entry 26: 2020-02-17, Monday](#id-section26)
     * [Entry 27: 2020-02-18, Tuesday](#id-section27)
     * [Entry 28: 2020-02-19, Wednesday](#id-section28)
     * [Entry 29: 2020-02-20, Thursday](#id-section29)
    * [Entry 30: 2020-02-21, Friday](#id-section30)
    * [Entry 31: 2020-02-24, Monday](#id-section31)
    * [Entry 32: 2020-02-25, Tuesday](#id-section32)
     * [Entry 33: 2020-02-26, Wednesday](#id-section33)
    * [Entry 34: 2020-02-27, Thursday](#id-section34)
    * [Entry 35: 2020-02-28, Friday](#id-section35)
    * [Entry 36: 2020-03-02, Monday](#id-section36)
     * [Entry 37: 2020-03-03, Tuesday](#id-section37)
     * [Entry 38: 2020-03-04, Wednesday](#id-section38)
    * [Entry 39: 2020-03-05, Thursday](#id-section39)
    * [Entry 40: 2020-03-06, Friday](#id-section40)
    * [Entry 41: 2020-03-09, Monday](#id-section41)
    * [Entry 42: 2020-03-10, Tuesday](#id-section42)
     * [Entry 43: 2020-03-11, Wednesday](#id-section43)
    * [Entry 44: 2020-03-12, Thursday](#id-section44)
    * [Entry 45: 2020-03-13, Friday](#id-section45)
    * [Entry 46: 2020-03-16, Monday](#id-section46)
    * [Entry 47: 2020-03-17, Tuesday](#id-section47)
     * [Entry 48: 2020-03-18, Wednesday](#id-section48)
    * [Entry 49: 2020-03-19, Thursday](#id-section49)
    * [Entry 50: 2020-03-20, Friday](#id-section50)
    * [Entry 51: 2020-03-23, Monday](#id-section51)
     * [Entry 52: 2020-03-24, Tuesday](#id-section52)
     * [Entry 53: 2020-03-25, Wednesday](#id-section53)
     * [Entry 54: 2020-03-26, Thursday](#id-section54)
    * [Entry 55: 2020-03-27, Friday](#id-section55)
    * [Entry 56: 2020-03-30, Monday](#id-section56)
     * [Entry 57: 2020-03-31, Tuesday](#id-section57)
     * [Entry 58: 2020-04-01, Wednesday](#id-section58)
     * [Entry 59: 2020-04-02, Thursday](#id-section59)
    * [Entry 60: 2020-04-03, Friday](#id-section60)
    * [Entry 61: 2020-04-06, Monday](#id-section61)
     * [Entry 62: 2020-04-07, Tuesday](#id-section62)
     * [Entry 63: 2020-04-08, Wednesday](#id-section63)
    * [Entry 64: 2020-04-09, Thursday](#id-section64)
    * [Entry 65: 2020-04-10, Friday](#id-section65)
    * [Entry 66: 2020-04-13, Monday](#id-section66)
    * [Entry 67: 2020-04-14, Tuesday](#id-section67)
    * [Entry 68: 2020-04-15, Wednesday](#id-section68)
     * [Entry 69: 2020-04-16, Thursday](#id-section69)
    * [Entry 70: 2020-04-17, Friday](#id-section70)
    * [Entry 71: 2020-04-20, Monday](#id-section71)
    * [Entry 72: 2020-04-21, Tuesday](#id-section72)
    * [Entry 73: 2020-04-22, Wednesday](#id-section73)
    * [Entry 74: 2020-04-23, Thursday](#id-section74)
    * [Entry 75: 2020-04-24, Friday](#id-section75)
    * [Entry 76: 2020-04-27, Monday](#id-section76)
     * [Entry 77: 2020-04-28, Tuesday](#id-section77)
     * [Entry 78: 2020-04-29, Wednesday](#id-section78)
    * [Entry 79: 2020-04-30, Thursday](#id-section79)
    * [Entry 80: 2020-05-01, Friday](#id-section80)
    * [Entry 81: 2020-05-04, Monday](#id-section81)
    * [Entry 82: 2020-05-05, Tuesday](#id-section82)
    * [Entry 83: 2020-05-06, Wednesday](#id-section83)
    * [Entry 84: 2020-05-07, Thursday](#id-section84)
    * [Entry 85: 2020-05-08, Friday](#id-section85)

------
<div id='id-section1'/>   

### Entry 1: 2020-01-01, Wednesday.   



------
<div id='id-section2'/>   

### Entry 2: 2020-01-02, Thursday.   



------
<div id='id-section3'/>   

### Entry 3: 2020-01-03, Friday.   



------
<div id='id-section4'/>   

### Entry 4: 2020-01-06, Monday.   



------
<div id='id-section5'/>   

### Entry 5: 2020-01-07, Tuesday.   



------
<div id='id-section6'/>   

### Entry 6: 2020-01-08, Wednesday.   



------
<div id='id-section7'/>   

### Entry 7: 2020-01-09, Thursday.   



------
<div id='id-section8'/>   

### Entry 8: 2020-01-10, Friday.   



------
<div id='id-section9'/>   

### Entry 9: 2020-01-13, Monday.   



------
<div id='id-section10'/>   

### Entry 10: 2020-01-14, Tuesday.   



------
<div id='id-section11'/>   

### Entry 11: 2020-01-15, Wednesday.   



------
<div id='id-section12'/>   

### Entry 12: 2020-01-16, Thursday.   



------
<div id='id-section13'/>   

### Entry 13: 2020-01-17, Friday.   



------
<div id='id-section14'/>   

### Entry 14: 2020-01-20, Monday.   



------
<div id='id-section15'/>   

### Entry 15: 2020-01-21, Tuesday.   



------
<div id='id-section16'/>   

### Entry 16: 2020-01-22, Wednesday.   

# Getting Started

* To login:
```
   bblack@pbio381.uvm.edu
   bblack@pbio381.uvm.edu password: (same as login)
```

* To see the full path to your current directory, use the pwd command:
```
  [srkeller@pbio381 ~]$ pwd
  /users/s/r/srkeller
  [srkeller@pbio381 ~]$
```
  
  So, the full path to my working directory on the server is: 
  ```
  /users/s/r/bblack/
 ```
* Github comands: 
1. Clone repo:
```
 git clone https://github.com/bblack2019/Ecological-Genomics-2020.git
```
2. Save add changes to repo, commit them, and push them to server:
 ```
 [srkeller@pbio381 mydata]$ git add --all .
 [srkeller@pbio381 mydata]$ git commit -m "comment about your commit"
 [srkeller@pbio381 mydata]$ git push
````
### To see the full path to your current directory, use the `pwd` command:
```
[srkeller@pbio381 ~]$ pwd
 /users/s/r/srkeller
 [srkeller@pbio381 ~]$ 
 ```
### You should make a new folder (aka, a directory, or “dir”) using the `mkdir` command. Name this folder `mydata`
```
[srkeller@pbio381 ~]$ mkdir mydata
```
* Make also a directory for `myscripts` and `myresults` 

### We can then use the `ll` command to list out the contents of any folders and files:
```
[srkeller@pbio381 Spring_2020]$ ll
total 12
drwxr-xr-x. 3 srkeller users   49 Jan 21 14:29 mydata
drwxr-xr-x. 3 srkeller users   47 Jan 21 14:29 myresults
drwxr-xr-x. 3 srkeller users 4096 Jan 21 12:51 myscripts
-rw-r--r--. 1 srkeller users 7929 Jan 21 12:28 SRKeller_PBIO381_2020_online_notebook.md
```
### We’ve placed the raw Illumina sequence files and a file containing the collection metadata: `/data/project_data/RS_ExomeSeq` 
```
srkeller@pbio381 ~]$ cd /data/project_data/RS_ExomeSeq
[srkeller@pbio381 mydata]$ ll
drwxr-xr-x. 3 srkeller users 31 Jan 16 13:16 fastq
drwxr-xr-x. 2 srkeller users 42 Jan 21 14:06 metadata
[srkeller@pbio381 mydata]$ 
```
* The fastq dir contains the paired-end Illumina sequence files, while the metadata dir contains the collection info. Use cd to navigate inside the metadata folder and ll; you should see a file called RS_Exome_metadata.txt.

### Use the `cp` command, followed by the filename, and the path to your destination (remember the ~ signals your home directory, and each subdirectory is then separated by a /): 
```
[srkeller@pbio381 metadata]$ cp RS_Exome_metadata.txt ~/<YourGithubRepoName>/mydata/
```

* We can use the `head` command to peek at the first 10 lines of data; or more generally, `head -n # filename` will show the first # lines of data:
```
[srkeller@pbio381 metadata]$ head RS_Exome_metadata.txt
[srkeller@pbio381 metadata]$ head -n 20 RS_Exome_metadata.txt
```
* Command `tail` also works for end of data

### Edge samples only for this project

* Use “generalized regular expression" , also known as the `grep` command.
* The option “-w” option tells `grep` to match the entire field has to be matched as-is.
```
[srkeller@pbio381 mydata]$ grep -w "E" RS_Exome_metadata.txt
AB      05      AB_05   TN      E       09002016        35.55297        83.49438        1812    57.7
AB      08      AB_08   TN      E       09002016        35.55212        83.49259        1785    44.3
AB      12      AB_12   TN      E       09002016        35.5389 83.49463        1750    36.8
[...]
```
### Eery UNIX command-line program has a built-in `man` page that you can call up to help you. Just type `man` and then the program name and it will give you the manual (small excerpt shown below).
```
[srkeller@pbio381 mydata]$ man grep
```

### “Piping”

* One of the most useful aspects of UNIX is the ability to take the output from one command and pass it along as standard input (termed ‘stdin’) into another command without having to save the intermediate files. 

* Uses character: `|` 

* Example: Say we wanted to create tally of many samples correspond to the Edge region. 
    * We can use `grep` to do the search and use the pipe (|) to send the results of grep to the `wc -l` command, which will tally up the number of lines.
```
[srkeller@pbio381 mydata]$ grep -w "E" RS_Exome_metadata.txt | wc -l
110
```
### Let’s find the # of unique populations that are contained in the edge

* The new commands are: `cut` to get just the column (also known as a “field”) containing the population code (the first col, “-f1”“), and `uniq` to collapse the number of rows to just 1 per population. Here’s the whole thing in one go:
```
[srkeller@pbio381 mydata]$ grep -w "E" RS_Exome_metadata.txt | cut -f1 | uniq 
AB
BFA
BRB
CR
CRA

[...]
```
### cominded all above to see how many unique edge pops: 
```
[srkeller@pbio381 mydata]$ grep -w "E" RS_Exome_metadata.txt | cut -f1 | uniq | wc -l
```

### Wildcard 

* Now, what if we want to do operations on multiple files at a time?

* There’s a way to do this quickly using the wildcard character `*`. With the wildcard, the `*` takes the place of any character, and in fact any length of characters.

### Let’s make a new folder called “metadata” and then move all the text files we’ve got so far into it:
```
[srkeller@pbio381 mydata]$ mkdir metadata
[srkeller@pbio381 mydata]$ mv *txt metadata/
[srkeller@pbio381 mydata]$ ll metadata/
```

### Remove command: 

* You can remove files and folders with the `rm` command. 

   * UNIX will not ask if you really mean it before getting rid of it forever(!), so this can be dangerous if you’re not paying attention.
   
* As an example, let’s use our `grep` command to pull out the samples that belong to the “AB”" population and save it to a new file called`metatdata/AB.txt`. But perhaps we later decide we’re not going to work with those samples, so we use `rm` to delete that file:
```
[srkeller@pbio381 mydata]$ cd metadata/
[srkeller@pbio381 mydata]$ rm AB.txt 
```
* NOTE: this is equivalent to `rm metadata/AB.txt`

### Adjust defaul settings for remove `rm`: 
1. `cd` to your home directory (~/
2. List all the files, including “hidden” ones that aren’t usually shown. To do this, use `ll -a`
3. Look for a file called “.bashrc” — this contains your settings for how you interact with the server when you log in.
4. We’re going to open this file and edit it to add a setting to request that `rm` confirms deletion with us. To edit text files on the fly in UNIX, you can use the built-in text editor, “vim”: `vim .bashrc`
5.
```
# .bashrc

  # Source global definitions
  if [ -f /etc/bashrc ]; then
          . /etc/bashrc
  fi

  # Uncomment the following line if you don't like systemctl's auto-paging feature:
  # export SYSTEMD_PAGER=

  # User specific aliases and functions
```
6. Use your arrow key to move your cursor down to the last line, below “”# User specific aliases and functions" — this is where we’re going to insert our new function.
7. By default, vim is in read-only mode when it opens files. To go into edit mode, press your “i” key (for “insert”). You are now able to make changes to the file.
8. Add the following text on a new line directly below the “# User specific…” line:
```
alias rm='rm -i'
```
9. Your file should now look like this:
```
# .bashrc

  # Source global definitions
  if [ -f /etc/bashrc ]; then
          . /etc/bashrc
  fi

  # Uncomment the following line if you don't like systemctl's auto-paging feature:
  # export SYSTEMD_PAGER=

  # User specific aliases and functions

  alias rm='rm -i'

```
10. You’re now ready to escape out of edit mode (hit the `escape` key), write (save) your changes (type `:w`), and quit vim (type `:q`). You can also combine this into a single command `:wq`

11. These changes won’t take effect until you log out (type exit to log out of the server). But from now on, every time you log in, the server will remember that you want a reminder before deleting any of your work.

### Add all chnages to repo and get with above commands. 


------
<div id='id-section17'/>   

### Entry 17: 2020-01-23, Thursday.   



------
<div id='id-section18'/>   

### Entry 18: 2020-01-24, Friday.   



------
<div id='id-section19'/>   

### Entry 19: 2020-01-27, Monday.   



------
<div id='id-section20'/>   

### Entry 20: 2020-01-28, Tuesday.   



------
<div id='id-section21'/>   

### Entry 21: 2020-01-29, Wednesday.   

### Red Spruce Background info 

* Red spruce is a coniferous tree that plays a prominent role in montane communities throughout the Appalachians. It thrives in the cool, moist climates of the high elevation mountains of the Apppalachians and northward along the coastal areas of Atlantic Canada.

* One region where populations are particular vulnerable to climate change is in the low-latitude trailing edge of the range, from Maryland to Tennessee, where populations are highly fragmented and isolated on mountaintops. These “island” populations are remnants of spruce forests that covered the southern U.S. glaciers extended as far south as Long Island, NY. As the climate warmed at the end of the Pleistocene (~20K years ago), red spruce retreated upward in elevation to these mountaintop refugia, where they are now highlty isolated from other such stands and from the core of the range further north.

* A goal of our study is to better understand the genetic resource represented by these fragemented edge populations, and to use that information to help inform conservation biologists working to restore red spruce in this region. A close partner in this effort is the Nature Conservancy and the Central Appalachian Spruce Restoration Initiative (CASRI) – a multi-partner group dedicated to restoring and enhancing red spruce populations to promote their resilience under climate change.


* Goals: 
1. characterize the genetic diversity and population structure across the range
2. identify regions of the genome that show evidence of positive selection in response to climate gradients
3. to map the genetic basis of climate adaptive phenotypes

* We hope to use this information to inform areas of the range most likely to experience climate maladaptation, and to help guide mitigation strategies.

* Experimental design: 
   * In 2017, we collected seeds and needle tissue from 340 mother trees at 65 populations spread throughout the range. We extracted whole genomic DNA from needles to use for exome capture sequencing.
   * Sample size in the edge region = 110 mother trees from 23 populations.
   * Exome capture was designed based on transcriptomes from multiple tissues and developmental stages in the related species, white spruce (P. glauca).
   * Bait design used 2 transcriptomes previously assembled by Rigault et al. (2011) and Yeaman et al. (2014).
   * A total of 80,000 120bp probes were designed, including 75,732 probes within or overlapping exomic regions, and an additional 4,268 probes in intergenic regions.
   * Each probe was required to represent a single blast hit to the P. glauca reference genome of at least 90bp long and 85% identity, covering 38,570 unigenes.
   * Libraries were made by random mechanical shearing of DNA (250 ng -1ug) to an average size of 400 bp followed by end-repair reaction, ligation of an adenine residue to the 3’-end of the blunt-end fragments to allow the ligation of barcoded adapters, and PCR-amplification of the library. SureSelect probes (Agilent Technologies: Santa Clara, CA) were used for solution-based targeted enrichment of pools of 16 libraries, following the SureSelectxt Target Enrichment System for Illumina Paired-End Multiplexed Sequencing Library protocol.
   * Libraries were sequenced on a single run of a Illumina HiSeq X to generate paired-end 150-bp reads.
   
### Pipeline 

1. Visualize, Clean, Visualize
* Visualize the quality of raw data (Program: FastQC)

* Clean raw data (Program: Trimmomatic)

* Visualize the quality of cleaned data (Program: FastQC)

2. Calculate #’s of cleaned, high quality reads going into mapping

3. Map (a.k.a. Align) cleaned reads from each sample to the reference assembly to generate sequence alignment files (Program: bwa, Input: .fastq, Output: .sam).

4. Remove PCR duplicates identified during mapping, and calculate alignment statistics (% of reads mapping succesully, mapping quality scores, average depth of coverage per individual)

### Visualize, Clean, Visualize protocol 


------
<div id='id-section22'/>   

### Entry 22: 2020-01-30, Thursday.   



------
<div id='id-section23'/>   

### Entry 23: 2020-01-31, Friday.   



------
<div id='id-section24'/>   

### Entry 24: 2020-02-03, Monday.   



------
<div id='id-section25'/>   

### Entry 25: 2020-02-04, Tuesday.   



------
<div id='id-section26'/>   

### Entry 26: 2020-02-05, Wednesday.   

#### Learning Objectives
1.	Review our progress on read cleaning and visualizing QC
2.	Start mapping (a.k.a. aligning) each set of cleaned reads to a reference genome
3.	Visualize sequence alignment files
4.	Process our sam files by
*  converting to binary (bam) format and sorting by coordinates
*  removing PCR duplicates
*  indexing for fast future lookup
5.	Calculate mapping statistics to assess quality of the result
6.	Learn how to put separate bash scripts into a “wrapper” that runs them all

#### Wget lets the server talk to the internet, a command that downloads files from the web

```
cd /data/project_data/RS_ExomeSeq/ReferenceGenomes/
wget "http://plantgenie.org/Picea_abies/v1.0/FASTA/GenomeAssemblies/Pabies1.0-genome.fa.gz"

```
Rather than trying to map to the entire 19.6 Gbp reference, we first subsetted the P. abies reference to include just the contigs that contain one or more probes from our exon capture experiment. For this, we did a BLAST search of each probe against the P. abies reference genome, and then retained all scaffolds that had a best hit.
*  This reduced reference contains:
*  668,091,227 bp (~668 Mbp) in 33,679 contigs
*  The mean (median) contig size is 10.5 (12.9) kbp
*  The N50 of the reduced reference is 101,375 bp
*  The indexed reduced reference genome to use for your mapping is on our server here: 

```
/data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa

```
N50- Metric of the state of the assembly. How much of the genome is assembled into big contigs than the smaller number? How far you must keep going until you hit 334mb. Smallest contig at which the sum of the total contig. it gives you a sense of how mature the genome assembly is. Gives you more spatial information. Is the length in base pairs, so bigger number is actually better

#### Short scripts

* First, we want to specify the population of interest and the paths to the input and output directories. We can do this by defining variables in bash, like so:
* Set your repo address here – double check yours carefully!
* myrepo="/users/s/r/srkeller/Ecological_Genomics/Spring_2020"
* Each student gets assigned a population to work with: mypop="YOURPOP""
* Directory with your pop-specific demultiplexed fastq files
* input="/data/project_data/RS_ExomeSeq/fastq/edge_fastq/pairedcleanreads/${mypop}"
* Output dir to store mapping files (bam)
* output="/data/project_data/RS_ExomeSeq/mapping"

#### mypipeline.sh

#!/bin/bash

#### we'll use this as a wrapper to run our different mapping scripts

#### path to my repo:
myrepo="/users/s/n/bblack/Ecological-Genomics-2020"

#### My population:
```
mypop="name"

#Directory to our cleaned and paired reads:

input="/data/project_data/RS_ExomeSeq/fastq/edge_fastq/pairedcleanreads/${mypop}"

#Directory to store the output of our mapping
output="/data/project_data/RS_ExomeSeq/mapping"

#run mapping.sh

source ./mapping.sh

#run the post processing steps

source ./process_bam.sh
```

#### For mapping, 
* Program bwa: very efficient read mapper. Lots of others exist and can be useful to explore for future datasets. 
* Tried severalfor  exome data
  *bwa seems to be the best
* Let’s write a bash script called mapping.sh that calls the R1 and R2 reads for each individual in our population, and uses the bwa-mem algorithm to map reads to the reference genome. We can test this out using one sample (individual) at a time, and then once the syntax is good and the bugs all worked out, we can scale this up to all the inds in our popuations. The basic bwa command we’ll use is below. Think about how we should write this into a loop to call all the fastq files for our population of interest…(hint, look back at the trim_loop.sh script)
```
bwa mem -t 1 -M ${ref} ${forward} ${reverse} > ${output}/BWA/${name}.sam
where
-t 1 is the number of threads, or computer cpus to use (in this case, just 1)
-M labels a read with a special flag if its mapping is split across >1 contig
-${ref} specifies the path and filename for the reference genome
${forward} specifies the path and filename for the cleaned and trimmed R1 reads 
${reverse} specifies the path and filename for the cleaned and trimmed R2 reads 
>${output}/BWA/${name}.sam  directs the .sam file to be saved into a directory called BWA
```
#### Script for mapping
```
#!/bin/bash

#this script will run the read mapping using "bwa" program

ref="/data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa"

#write a loop to map each individual within my population

for forward in ${input}*_R1.cl.pd.fq

do
  reverse=${forward/_R1.cl.pd.fq/_R2.cl.pd.fq}
  f=${forward/_R1.cl.pd.fq/}
  name=`basename ${f}`
  bwa mem -t 1 -M ${ref} ${forward} ${reverse} > ${output}/BWA/${name}.sam
done
```


#### Script for processing
```
#!/bin/bash

#this is where our output sam files are going to get converted into binary format (bam)
#then we are going to sort the bam files, remove the PCR duplicates and index them

#first, lets convert sam to bam

for f in ${output}/BWA/${mypop}*.sam

do

  out=${f/.sam/}
  sambamba-0.7.1-linux-static view -S --format=bam ${f} -o ${out}.bam
  samtools sort ${out}.bam -o ${out}.sorted.bam 
  
done
```
#now lets remove the PCR duplicates from our bam files
```
for file in ${output}/BWA/${mypop}*.sorted.bam

do

  f=${file/.sorted.bam/}
  sambamba-0.7.1-linux-static markdup -r -t 1 ${file} ${f}.sorted.rmdup.bam
  
done
```
```
cd Ecological-Genomics-2020/
cd myresults/
ll
cd fastqc
cd ..
cd /data/project_data/RS_ExomeSeq/fastq/edge_fastq/
ll
cd pairedcleanreads/
cd /
pwd
ll
exit 

```

```
cd Ecological-Genomics-2020/
ll /data/project_data/RS_ExomeSeq/fastq/edge_fastq/pairedcleanreads/
# My population
mypop="KOS"
#Directory to our cleaned and paired reads
input="/data/project_data/RS_ExomeSeq/fastq/edge_fastq/pairedcleanreads/${mypop}"
echo ${input}
output="/data/project_data/RS_ExomeSeq/mapping"
bwa
ll /data/project_data/RS_ExomeSeq/ReferenceGenomes/
pwd
ll /data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa
echo ${mypop}
mypop="GFM_01"
ref="/data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa"
#write a loop to map eac individual within my population
 for forward in ${input}*_R1.cl.pd.fq; do reverse=${forward/_R1.cl.pd.fq/_R2.cl.pd.fq}; f=${forward/_R1.cl.pd.fq/}; name=`basename ${f}`; bwa mem -t 1 -M ${ref} ${forward} ${reverse} > ${output}/BWA/${name}.sam; done
cd myscripts/
ll
chmod u+x mapping.sh
chmod u+x mypipeline.sh
chmod u+x process_bam.sh
ll
git pull
screen
top
screen -r
exit
```
#### Wildcard

```
ll /data/project_data/RS_ExomeSeq/mapping/BWA/
ll /data/project_data/RS_ExomeSeq/mapping/BWA/GFM*
ll /data/project_data/RS_ExomeSeq/mapping/BWA/GFM*bai
ll /data/project_data/RS_ExomeSeq/mapping/BWA/GFM*.ba
ll /data/project_data/RS_ExomeSeq/mapping/BWA/GFM*bam
exit

```
<div id='id-section27'/>   

### Entry 27: 2020-02-06, Thursday.   



------
<div id='id-section28'/>   

### Entry 28: 2020-02-07, Friday.   



------
<div id='id-section29'/>   

### Entry 29: 2020-02-10, Monday.   



------
<div id='id-section30'/>   

### Entry 30: 2020-02-11, Tuesday.   



------
<div id='id-section31'/>   

### Entry 31: 2020-02-12, Wednesday.   

#### Learning Objectives for 02/12/20
1.	Review our progress on mapping
2.	Calculate mapping statistics to assess quality of the result (how many of the reads mapped uniquely, how many reads on average cover the site (depth) 
3.	Visualize sequence alignment files
4.	Introduce use of genotype-likelihoods for analyzing diversity in low coverage sequences. Embracing statiscal uncertainty in alignment
5.	Use the ‘ANGSD’ progranm to calculate diversity stats, Fsts, and PCA
Sam file is human readeable
```
/data/project_data/RS_ExomeSeq/mapping/BWA/
ll
ll GFM*sam
head GFM_01.sam
tail GFM_01.sam
```
* A row of data consists of a potential data. The first row is the reads name. the next number is a flag (e.g 147 can be known by a SAM decode- means the read was paired. It was mapped together as forward and reverse. It’s the second read in the pair. It’s the reverse read.r)
* When a read maps to more than one spot its not a primary alignment the next is the contig that the read mapped to (MA_18732) the left most position of the read. 60 is the base quality score, 6 orders of magnitude, it’s a very strong match- mapping quality (MAQ) 
Followed by the actual sequence and the quality scores

* A SAM file is a tab delimited text file that stores information about the alignment of reads in a FASTQ file to a reference genome or transcriptome. For each read in a FASTQ file, there’s a line in the SAM file that includes the read, aka. query, name, a FLAG (number with information about mapping success and orientation and whether the read is the left or right read)
* the reference sequence name to which the read mapped 	the leftmost position in the reference where the read mapped the mapping quality (Phred-scaled)
* a CIGAR string that gives alignment information (how many bases Match (M), where there’s an Insertion (I) or Deletion (D)) an ‘=’, mate position, inferred insert size (columns 7,8,9), the query sequence and Phred-scaled quality from the FASTQ file (columns 10 and 11),
* Lots of good information in TAGS at the end, if the read mapped, including whether it is a unique read (XT:A:U), the number of best hits (X0:i:1), the number of suboptimal hits (X1:i:0).
The left (R1) and right (R2) reads alternate through the file. SAM files usually have a header section with general information where each line starts with the ‘@’ symbol. SAM and BAM files contain the same information; SAM is human readable and BAM is in binary code and therefore has a smaller file size.
Find the official Sequence AlignMent file documentation can be found here or more officially.
* Here’s a SAM FLAG decoder by the Broad Institute. Use this to decode the second column of numbers
Useful for getting info on the sam files
How can we get a summary of how well our reads mapped to the reference?
* We can use the program samtools Written by Heng Li, the same person who wrote bwa. It is a powerful tool for manipulating sam/bam files.
* The command flagstat gets us some basic info on how well the mapping worked:
```
samtools flagstat GFM_01.sam
filename sorted.rmdup.bam
filename sorted.rmdup.bai
writing bamstats scripts
```
```
#!/bin/bash
```
#### Set repo
```
myrepo="/users/s/n/bblack/Ecological-Genomics-2020"

mypop="GFM"

output="/data/project_data/RS_ExomeSeq/mapping"

echo "num.reads R1 R2 Paired MateMapped Singletons MateMappedDiffChr" > ${myrepo}/myresults/${mypop}.flagstats.txt

for file in ${output}/BWA/${mypop}*sorted.rmdup.bam
do
  f=${file/.sorted.rmdup.bam/}
  name=`basename ${f}`
  echo ${name} >> ${myrepo}/myresults/${mypop}.names.txt
  samtools flagstat ${file} | awk 'NR>=6&&NR<=12 {print $1}' | column -x 
done >> ${myrepo}/myresults/${mypop}.flagstats.txt
```
#### Calculate depth of coverage from our bam files
```
for file in ${output}/BWA/${mypop}*sorted.rmdup.bam
do
  samtools depth ${file} | awk '{sum+=$3} END {print sum/NR}'
 done >> ${myrepo}/myresults/${mypop}.coverage.txt
 ```
*While that’s running, we can take a look at one of our alignment files (sam or bam) using an integrated viewed in samtools called tview. To use it, simply call the program and command, followed by the sam/bam file you want to view and the path to the reference genome. 
*For example:
```
samtools tview /data/project_data/RS_ExomeSeq/mapping/BWA/AB_05.sorted.rmdup.bam /data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa
```
* Genotype-free population genetics using genotype likelihoods
* A growing movement in popgen analysis of NGS data is embracing the use of genotype likelihoods to calculate stats based on each individual having a likelihood (probability) of being each genotype.
* A genotype likelihood (GL) is essentially the probability of observing the sequencing data (reads containing a particular base), given the genotype of the individual at that site.
* These probabilities are modeled explicitly in the calculation of population diversty stats like pi, Tajima’s D, Fst, PCA, etc…; thus not throwing out any precious data, but also making fewer assumptions about the true (unknown) genotype at each locus
* We’re going to use this approach with the program ‘ANGSD’, which stands for ‘Analysis of Next Generation Sequence Data’
* This approach was pioneered by Rasmus Nielsen, published originally in Korneliussen et al. 2014.

1.	Create a list of bam files for the samples you want to analyze
2.	Estimate genotype likelihoods (GL’s) and allele frequencies after filtering to minimize noise
3.	Use GL’s to:
* estimate the site frequency spectrum (SFS)
* estimate nucleotide diversities (Watterson’s theta, pi, Tajima’s D, …)
* estimate Fst between all populations, or pairwise between sets of populations
* perform a genetic PCA based on estimation of the genetic covariance matrix (this is done on the entire set of Edge ind’s)

#### Writing ANGSD script
```
myrepo="/users/s/n/bblack/Ecological-Genomics-2020"

mkdir ${myrepo}/myresults/ANGSD

output="${myrepo}/myresults/ANGSD"

mypop="KOS"

ls /data/project_data/RS_ExomeSeq/mapping/BWA/${mypop}_*sorted.rm*.bam >${output}/${mypop}_bam.list 

REF="/data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa"
```
#### Estimating GL's and allele frequencies for all sites with ANGSD
```
ANGSD -b ${output}/${mypop}_bam.list \
-ref ${REF} -anc ${REF} \
-out ${output}/${mypop}_allsites \
-nThreads 1 \
-remove_bads 1 \
-C 50 \
-baq 1 \
-minMapQ 20 \
-minQ 20 \
-setMinDepth 3 \
-minInd 2 \
-setMinDepthInd 1 \
-setMaxDepthInd 17 \
-skipTriallelic 1 \
-GL 1 \
-doCounts 1 \
-doMajorMinor 1 \
-doMaf 1 \
-doSaf 1 \
-doHWE 1 \
-SNP_pval 1e-6
```
* Baq1- Discounts snps close to  indels
* Min q- individual bases have to have a small prob of being called incorrectly
* Set min depth-At least 3 reads at the site in order to keep it
setMaxDepthInd- We set a max depth for individual we can accept to remove PCR duplicates that is excess. When there are duplicates of a gene. If the reads are stacking up together its not good. 
* Skiptriallelic- We skip sites that have 3 or more alleles
* Genotype likelihood model GL 1 
* doCounts- Counts of allele at each site
* Identify major (most frequent-ancestral allele) and minor allele (rare or derived allel)
* doSaf 1- Generates statistical analysis of alleles



```
cd /data/project_data/RS_ExomeSeq/mapping/BWA/
ll
ll GFM*sam
head GFM_01.sam
tail GFM_01.sam
samtools flagstat GFM_01.sam
cd ~/Ecological-Genomics-2020/
pwd
/users/s/n/bblack/Ecological-Genomics-2020
mypop="GFM"
myrepo="/users/s/n/snnadi/Ecological-Genomics-2020"
output="/data/project_data/RS_ExomeSeq/mapping"
echo "num.reads R1 R2 Paired MateMapped Singletons MateMappedDiffChr" > ${myrepo}/myresults/${mypop}.flagstats.txt
ll
cd myresults/
ll
cat KOS.flagstats.txt
for file in ${output}/BWA/${mypop}*sorted.rmdup.bam; do  f=${file/.sorted.rmdup.bam/}; name=`basename ${f}`
echo ${name} >> ${myrepo}/myresults/${mypop}.names.txt
samtools flagstat ${file} | awk 'NR>6&&NR<=12 {print $1}' | column -x
done >> ${myrepo}/myresults/${mypop}.flagstats.txt
cat ${output}/BWA/${mypop}*sorted.rmdup.bam
ll ${output}/BWA/${mypop}*
less ${myrepo}/myresults/${mypop}.flagstats.txt
rm ${myrepo}/myresults/${mypop}.flagstats.txt
cat ${myrepo}/myresults/${mypop}.flagstats.txt
cat ${myrepo}/myresults/${mypop}.names.txt
for file in ${output}/BWA/${mypop}*sorted.rmdup.bam; do  
samtools depth ${file} | awk '{sum+=$3} END {print sum/NR}'
done >> ${myrepo}/myresults/${mypop}.coverage.txt
ll /data/project_data/RS_ExomeSeq/mapping/BWA/
samtools tview /data/project_data/RS_ExomeSeq/mapping/BWA/KOS_01.sorted.rmdup.bam /data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa
samtools tview /data/project_data/RS_ExomeSeq/mapping/BWA/KOS_02.sorted.rmdup.bam /data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa
pwd
/users/s/n/bblack/Ecological-Genomics-2020
myrepo=/users/s/n/snnadi/Ecological-Genomics-2020
mkdir ${myrepo}/myresults/ANGSD
output="${myrepo}/myresults/ANGSD"
mypop="GFM"
ls /data/project_data/RS_ExomeSeq/mapping/BWA/${mypop}_*sorted.rm*.bam >${output} /${mypop}_bam.list
ll
cd myresults
ll
cd ANGSD/
ll
cat GFM_bam.list
REF="/data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa"
screen -s ANGSD
screen -r ANGSD
exit

```

------
<div id='id-section32'/>   

### Entry 32: 2020-02-13, Thursday.   



------
<div id='id-section33'/>   

### Entry 33: 2020-02-14, Friday.   



------
<div id='id-section34'/>   

### Entry 34: 2020-02-17, Monday.   



------
<div id='id-section35'/>   

### Entry 35: 2020-02-18, Tuesday.   



------
<div id='id-section36'/>   

### Entry 36: 2020-02-19, Wednesday.   

#### Learning Objectives for 02/19/20

1.	Appreciate the difference between the unfolded and the folded SFS (which should we use?)
2.	Calculate diversity stats for our focal pops (SFS, Num sites, Frequency of SNPs, theta-W, pi, Tajima’s D)
3.	Visualize results in R and share to google drive
4.	Introduce Genome-Wide Association Studies (GWAS) in ANGSD using genotype probabilities
5.	Do GWAS on seedling heights

#### The unfolded vs. folded SFS
* The big difference here is whether we are confident in the ancestral state of each variable site (SNP) in our dataset
* If we know the anestral state, then the best info is contained in the unfolded spectrum, which shows the frequency histogram of how many derived loci are rare vs. common bins in the unfolded spectra go from 0 to 2N – why?
* When you don’t know the ancestral state confidently, you can make the SFS based on the minor allele (the less frequent allele; always < 0.5 in the population).
* bins in the folded spectra go from 0 to 1N – why?
  * Essentially, the folded spectra wraps the SFS around such that high frequency “derived” alleles are put in the small bins (low minor allele freq).

#### Calculate SFS and diversity stats
* In your myscripts folder, let’s revise the script ANGSD_mypop.sh to work on the folded SFS
The first part will be identical as last time, except: 1. Let’s change the -out name to -out ${output}/${mypop}_outFold 2. Take out the HWE test (not necessary to run again) and replace it with fold 1
* For info on using the folded spectrum, see the ANGSD manual page for Theta stats under “folded”.
The above command generated the site allele frequencies, which we can now use to estimate the folded SFS for your pop
realSFS ${output}/${mypop}_outFold.saf.idx -maxIter 1000 -tole 1e-6 -P 1 > ${output}/${mypop}_outFold.sfs
* Once you have the SFS, you can estimate the theta diversity stats:
We do this by running ANGSD again, as above, but this time we include the -pest flag which uses the SFS we estimated previously as a prior on the allele frequency estimation. We also include the doThetas flag, and of course the fold 1 to tell ANGSD we want the diversities based on the folded SFS
* So, we’re copying our previous set of ANGSD commands to make a new entry and adding:
```
-pest ${output}/${mypop}_outFold.sfs \
-doSaf 1 \
-doThetas 1 \
-fold 1
```
```
thetaStat do_stat ${output}/${mypop}_outFold.thetas.idx
The first column of the results file (${mypop}.thetas.idx.pestPG) is formatted a bit funny and we don’t really need it. We can use the cut command to get rid of it if we want to, or just ignore it.
cut -f2- ${mypop}.thetas.idx.pestPG > ${mypop}.thetas
``` 

```
myrepo=/users/s/n/snnadi/Ecological-Genomics-2020
output="${myrepo}/myresults/ANGSD"
mypop="KOS"
REF="/data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa"
ANGSD -b ${output}/${mypop}_bam.list \
-ref ${REF} -anc ${REF} \
-out ${output}/${mypop}_folded_allsites \
-nThreads 1 \
-remove_bads 1 \
-C 50 \
-baq 1 \
-minMapQ 20 \
-minQ 20 \
-setMinDepth 3 \
-minInd 2 \
-setMinDepthInd 1 \
-setMaxDepthInd 17 \
-skipTriallelic 1 \
-GL 1 \
-doCounts 1 \
-doMajorMinor 1 \
-doMaf 1 \
-doSaf 1 \
-fold 1 
realSFS ${output}/${mypop}_folded_allsites.saf.idx \
-maxIter 1000 -tole 1e-6 -P 1 \
> ${output}/${mypop}_outFold.sfs
ANGSD -b ${output}/${mypop}_bam.list \
-ref ${REF} -anc ${REF} \
-out ${output}/${mypop}_folded_allsites \
-nThreads 1 \
-remove_bads 1 \
-C 50 \
-baq 1 \
-minMapQ 20 \
-minQ 20 \
-setMinDepth 3 \
-minInd 2 \
-setMinDepthInd 1 \
-setMaxDepthInd 17 \
-skipTriallelic 1 \
-GL 1 \
-doCounts 1 \
-doMajorMinor 1 \
-doMaf 1 \
-doSaf 1 \
-fold 1 \
-pest ${output}/${mypop}_outFold.sfs \
-doThetas 1
thetaStat do_stat ${output}/${mypop}_folded_allsites.thetas.idx

```

*This is now ready to bring into R (transfer via git) to look at the mean per-site nucleotide diversity for your focal population. How does it compare to other populations? Also bring in your SFS for plotting, and to calculate the SNP frequency in your population.

#### RSTUDIO SCRIPTS
```
setwd("~/GitHub/Ecological-Genomics-2020/myresults")
list.files()

SFS <- scan("GFM_outFold.sfs")

sumSFS <- sum(SFS)

sumSFS

pctPoly = 100*(1-(SFS[1]/sumSFS))

plotSFS <- SFS[-c(1,length(SFS))]

barplot(plotSFS)

div <-read.table("GFM_folded_allsites.thetas.idx.pestPG")

colnames(div) =c("window","chrname","winscenter","tW","tP","tF","tH","tL","tajD","fulif","fuliD","fayH","zengsE","numSites")

div$tWpersite = div$tW/div$numSites
div$tPpersite = div$tP/div$numSites

pdf("KOS_diversity_stats.pdf")

# Share your population specific stats with the group
````


------
<div id='id-section37'/>   

### Entry 37: 2020-02-20, Thursday.   



------
<div id='id-section38'/>   

### Entry 38: 2020-02-21, Friday.   



------
<div id='id-section39'/>   

### Entry 39: 2020-02-24, Monday.   

#### Red spruce stress transcriptomics experiment

* The purpose of this experiment was to sample red spruce genetic variation from sites that were polarized into cool & wet vs. hot and dry based on historic climate and to assess the gene expression responses of individuals from these habitats in response to experimental treatments of heat and heat+drought.

####  Experimental Design:

1. Ten maternal families total; sample labels are “POP_FAM”

 * ASC_06, BRU_05, ESC_01, XBM_07, NOR_02, CAM_02, JAY_02, KAN_04, LOL_02, MMF_13

2. Two Source Climates (SourceClim:
 * HotDry (5 fams): ASC_06, BRU_05, ESC_01, XBM_07, NOR_02
 * CoolWet (5 fams): CAM_02, JAY_02, KAN_04, LOL_02, MMF_13
3. Experimental Treatments (Trt):
 * Control: watered every day, 16:8 L:D photoperiod at 23C:17C temps
 * Heat: 16:8 L:D photoperiod at 35C:26C temps (50% increase in day and night temps over controls)
 * Heat+Drought: Heat plus complete water witholding
4. Three time periods (Day):
 * Harvested tissues on Days 0, 5, and 10 (from one year to 6 months old seedlings) 
 * Extracted RNA from whole seedlings (root, stem, needle tissue)
 * Goal: Aimed for 5 biological reps per Trt x SourceClim x Day combo, but day5 had few RNA extractions that worked

# Realized sample replication after sequencing: N=76
* Just sequence data from one direction 
  * 3' tagging RNA data 
* 5N reps for each temperature type 
* Not all samples are equal number with enqual distrobution

#### Library prep and sequencing
* Samples were quantified for RNA concentration and quality on the Bioanalyzer
* Samples >1 ng/ul were sent to Cornell for 3’ tag sequencing (Great location for this type of data, good turn around time, been doing it the longest, How did you recieve the data?) 
* Library prep followed the LexoGen protocol and sequencing was on 1 lane of a NextSeq500 (1x86 bp reads)
* Samples were demultiplexed and named according to the convention: POP_FAM_TRT_DAY (Named like this) 
   * Keep files name all the same length, downstreams inforamtics easier. 
* UVM core facility 2-3x more expensive than other facilities. (Consultation was lower and quality as well). 
* If all fails: who pays? Contamination vs degreadation: taxa specific issues. 
   * look at other papers of similar extraction, protocols. 

#### What questions can we ask/address with this experimental design, with these data?
*Factors: 
 * Treatment: C, H, D 
 * Source climate: Hot/dry, Cool/Wet
 * Time: 0, 5, 10 days 

1. Do individuals from diverent climate have different gene expression to 
  * " at different conditions? (exp = source clim + treatment + (SC x trt)
  * " at different time points? (exp = time + source clim (time x SC) + fam)
2. Does souce climate provide different methods to deal with stress?
3. What particular genes are responsible for these responses? 
4. Expression cause a different from different alleles? 


#### Data Processing Pipeline:
1. FastQC on raw reads –> Trimmomatic (done!) –> FastQC on cleaned reads
2. Reference transcriptome:
```
/data/project_data/RS_RNASeq/ReferenceTranscriptome/Pabies1.0-all-cds.fna.gz
```
Downloaded from Congenie.org <- conifer resource 
3. 66,632 unigenes, consisting of 26,437 high-confidence gene models, 32,150 medium-confidence gene models, and 8,045 low-confidence gene models
4. Use Salmon to simulateously map reads and quantify abundance.
5. Import the data into DESeq2 in R for data normalization, visualization, and statistical tests for differential gene expression.

# Choose samples to visualize for quality (FastQC) and to map (Salmon)

* Ignoring `day` for the moment, there are 10 PopulationByFamily groups (`POP_XX`) and three treatment groups (`C`, `H`, and `D`), so 30 groups. If you each take two of these pop by treatment groups to process, we’ll have all the samples covered. `cd` into the `/data/project_data/RS_RNASeq/fastq` directory to view the files. Let’s write on the board each of our chosen groups.

* ASC (C,D,H), BRU (C,D,H), ETC...

My pop is: ESC D & H 
Script to focus on just this is 
```
/data/project_data/RS_RNAseq/fastq/ESC_01_D*R1.fastq.gz
/data/project_data/RS_RNAseq/fastq/ESC_01_H*R1.fastq.gz
```
#Clean the reads with Trimmomatic
```
#!/bin/bash

cd /data/project_data/RS_RNASeq/fastq/

########## Trimmomatic for single end reads

for R1 in *R1.fastq.gz  

do 
    echo "starting sample ${R1}"
    f=${R1/_R1.fastq.gz/}
    name=`basename ${f}`

    java -classpath /data/popgen/Trimmomatic-0.33/trimmomatic-0.33.jar org.usadellab.trimmomatic.TrimmomaticSE \
        -threads 1 \
        -phred33 \
         "$R1" \
         /data/project_data/RS_RNASeq/fastq/cleanreads/${name}_R1.cl.fq \
        ILLUMINACLIP:/data/popgen/Trimmomatic-0.33/adapters/TruSeq3-SE.fa:2:30:10 \
        LEADING:20 \
        TRAILING:20 \
        SLIDINGWINDOW:6:20 \
        HEADCROP:12 \
        MINLEN:35 
     echo "sample ${R1} done"
done 
```




------
<div id='id-section40'/>   

### Entry 40: 2020-02-25, Tuesday.   



------
<div id='id-section41'/>   

### Entry 41: 2020-02-26, Wednesday.   



------
<div id='id-section42'/>   

### Entry 42: 2020-02-27, Thursday.   



------
<div id='id-section43'/>   

### Entry 43: 2020-02-28, Friday.   



------
<div id='id-section44'/>   

### Entry 44: 2020-03-02, Monday.   



------
<div id='id-section45'/>   

### Entry 45: 2020-03-03, Tuesday.   



------
<div id='id-section46'/>   

### Entry 46: 2020-03-04, Wednesday.   

#### Learning Objectives for 3/4/20
* You should have installed the relevant programs in R.
* Map cleaned reads and quantify abundance simultaneously using Salmon.
* Assess mapping rate (Salmon log files); explore mapping to different reference trancript sets.
* Generate compiled counts matrix (all 76 samples) from individual quant.sf files using tximport.
* Move the data matrix to your machine (Fetch/WinSCP/SCP).
* Import data matrix and sample information into R and DESeq2
* Normalize, visualize and analyze expression data using DESeq2.

#### Use Salmon to quantify transcript abundance
* First step: Index the reference transcriptome. This only needs to be done once and has been done already, but here’s the code:

```
cd /data/project_data/RS_RNAseq/ReferenceTrancriptome/
salmon index -t Pabies1.0-all-cds.fna.gz -i Pabies_cds_index --decoys decoys.txt -k 31
```
Note that the -k flag sets the minimum acceptable length for a valid match between query (read) and the reference. This is a parameter that the creators of Salmon suggest can be made “slightly smaller” if the mapping rate is lower than expected. We may want to explore reducing -k.

* Second step: Start quantification! Below is the basic command for running the quantification from the documentation, Salmon tutorial How do we turn this into a for loop to process our samples? Let’s do it.

```
salmon quant -i transcripts_index -l <LIBTYPE> -r reads.fq --validateMappings -o transcripts_quant
```
The descriptions of all of the options can be found on the Salmon github page and by using the command salmon quant -h.

```
!#/bin/bash

cd /data/project_data/RS_RNASeq/fastq/cleanreads/

for file in JAY_02_H*.cl.fq

  do

    salmon quant -i /data/project_data/RS_RNASeq/ReferenceTranscriptome/Pabies_HC27_index \
	-l A \
	-r /data/project_data/RS_RNASeq/fastq/cleanreads/${file} \
	--validateMappings \
	-o /data/project_data/RS_RNASeq/salmon/cleanedreads/${file} 

  done 
  
  cd /data/project_data/RS_RNASeq/fastq/cleanreads/

for file in JAY_02_H*.cl.fq

  do

    salmon quant -i /data/project_data/RS_RNASeq/ReferenceTranscriptome/Pabies_HC_index \
	-l A \
	-r /data/project_data/RS_RNASeq/fastq/cleanreads/${file} \
	--validateMappings \
	-p 1 \
	--seqBias \
	-o /data/project_data/RS_RNASeq/salmon/HCmapping/${file} 

  done 
  
  
  cd /data/project_data/RS_RNASeq/fastq/cleanreads/

for file in JAY_02_H*.cl.fq

  do

    salmon quant -i /data/project_data/RS_RNASeq/ReferenceTranscriptome/Pabies_cds_index \
	-l A \
	-r /data/project_data/RS_RNASeq/fastq/cleanreads/${file} \
	--validateMappings \
	-p 1 \
	--seqBias \
	-o /data/project_data/RS_RNASeq/salmon/allmapping/${file} 

  done 
  ```

#### Explore mapping rate
For each sample mapped, you now have a directory with several output files including a log of the run. In that log, the mapping rate (% of reads mapped with sufficient quality) is reported. We can view the contents of the file using cat. We can also use grep (i.e., regular expressions) to pull out the mapping rate for all the samples. Though there’s probably a more elegant solution, here is one:

```
cd /data/project_data/RS_RNASeq/salmon/cleanreads/
cd /data/project_data/RS_RNASeq/salmon/allmapping/${file} 
grep -r --include \*.log -e 'Mapping rate'
cd JAY_02_C_10_TATGTC_R1.cl.fq
ll
head -n 10 quant.sf

```

------    
------
<div id='id-section47'/>   

### Entry 47: 2020-03-05, Thursday.   



------
<div id='id-section48'/>   

### Entry 48: 2020-03-06, Friday.   



------
<div id='id-section49'/>   

### Entry 49: 2020-03-09, Monday.   



------
<div id='id-section50'/>   

### Entry 50: 2020-03-10, Tuesday.   



------
<div id='id-section51'/>   

### Entry 51: 2020-03-11, Wednesday.   



------
<div id='id-section52'/>   

### Entry 52: 2020-03-12, Thursday.   



------
<div id='id-section53'/>   

### Entry 53: 2020-03-13, Friday.   



------
<div id='id-section54'/>   

### Entry 54: 2020-03-16, Monday.   



------
<div id='id-section55'/>   

### Entry 55: 2020-03-17, Tuesday.   



------
<div id='id-section56'/>   

### Entry 56: 2020-03-18, Wednesday.   

#### Learning Objectives for 3/17/20
* Recap on Mapping of 3’ RNA-seq data to transcriptome and assembly of data matrix
* Import data matrix and sample information into R and DESeq2
* Normalize, visualize and analyze expression data using DESeq2.

#### Troubleshooting and improving mapping rate
* Two weeks ago we wrote for loops to map our set of cleaned fastq files to the reference transcriptome. 
* We discovered that we had low mapping rates, ~2%! We did some troubleshooting in class. We further hypothesized that many of our reads were not mapping because the reference we had selected included only the coding region. 
* In working with 3’ RNAseq data, much of our sequencing effort is likely to be in the 3’ UTR (untranslated region). We concatenated the reference sequences for the coding (“cds”) and the 3’ UTR (“2kb downstream”) for each gene. 
* We then mapped to this new reference using salmon (as you had done before). Our mapping rate improved dramatically, ranging from 40-70% of reads mapping across samples, mean of 52%.

* As described in the last tutorial, after all samples have been mapped to the refenence, the next step is to assemble the counts matrix using tximport in R on the server to be able to move from read mapping with Salmon to differential gene expression (DGE) analysis with DESeq2. See previous tutorial for code to assemble counts matrix.

#### Import Counts Matrix and Sample ID tables into R and DESeq2
* Now we will work in R on our individual machines, each of us working with the complete data set (n=76, not just a subset of samples).

* Below is the scaffold (and a bit of code) for what we will be live coding today. Copy this into a new R document in RStudio. The idea is for you to not have to simultaneously annotate and accurately live code. Hopefully, this will also help you to better understand each step we do. Do feel free to add more notes and duplicate and experiment with scripts.

#### Set your working directory
* Go to session on Rstudio, click on 'set working directory' then choose 'RS_counts_samples'

#### Import the libraries that we're likely to need in this session
```
library(DESeq2)
library(dplyr)
library(tidyr)
library(ggplot2)
library(scales)
library(ggpubr)
library(wesanderson)
library(vsn)  ### First: BiocManager::install("vsn") AND BiocManager::install("hexbin")
```
#### Import the counts matrix
```
countsTable <- read.table("RS_cds2kb_countsMatrix.txt", header=TRUE, row.names=1)
head(countsTable)
dim(countsTable)
countsTableRound <- round(countsTable) * Need to round because DESeq wants only integers
head(countsTableRound)
```
#### Import the samples description table - links each sample to factors of the experimental design.
* Need the colClasses otherwise imports "day" as numeric which DESeq doesn't like, could alternatively change to d0, d5, d10
```
conds <- read.delim("RS_samples.txt", header=TRUE, stringsAsFactors = TRUE, row.names=1, colClasses=c('factor', 'factor', 'factor', 'factor'))
head(conds)
dim(conds)
```

#### Let's see how many reads we have from each sample:
```
colSums(countsTableRound)
mean(colSums(countsTableRound))
barplot(colSums(countsTableRound), las=3, cex.names=0.5,names.arg = substring(colnames(countsTableRound),1,13))
abline(h=mean(colSums(countsTableRound)), col="blue", lwd =2)
```
```
# Create a DESeq object and define the experimental design here with the tilde
# Filter out genes with few reads
# Run the DESeq model to test for differential gene expression: 1) estimate size factors (per sample), 2) estimate dispersion (per gene), 3) run negative binomial glm
# List the results you've generated
# Order and list and summarize results from specific contrasts
# Here you set your adjusted p-value cutoff, can make summary tables of the number of genes differentially expressed (up- or down-regulated) for each contrast
```

#### Data visualization
```
# MA plot
# PCA
# Counts of specific top gene! (important validatition that the normalization, model is working)
# Heatmap of top 20 genes sorted by pvalue

countsTable <- read.table("RS_cds2kb_countsMatrix.txt", header=TRUE, row.names=1)
head(countsTable)
dim(countsTable)
countsTableRound <- round(countsTable)
head(countsTableRound)
conds <- read.delim("RS_samples.txt", header=TRUE, stringsAsFactors = TRUE, row.names=1, colClasses=c('factor', 'factor', 'factor', 'factor'))
head(conds)
dim(conds)
grep("10", names(countsTableRound), value = TRUE)
day10countstable <- subset(countsTableRound, grep("10", names(countsTableRound), value = TRUE)) #doesn't work has to be logical

day10countstable <- countsTableRound %>% select(contains("10"))
dim(day10countstable)

conds10<- subset(conds, day=="10")
dim(conds10)
head(conds10)

colSums(countsTableRound)
mean(colSums(countsTableRound))
barplot(colSums(countsTableRound), las=3, cex.names=0.5,names.arg = substring(colnames(countsTableRound),1,13))
abline(h=mean(colSums(countsTableRound)), col="blue", lwd =2)

rowSums(countsTableRound)
mean(rowSums(countsTableRound))
median(rowSums(countsTableRound))

apply(countsTableRound,2,mean)

dds <- DESeqDataSetFromMatrix(countData = countsTableRound, colData = conds, 
                              design = ~ climate + day + treatment)
dim(dds)
dds <- dds[rowSums(counts(dds)) > 76]
dim(dds)

dds <- DESeq(dds)

resultsNames(dds)

res <- results(dds, alpha = 0.05)
res <- res[order(res$padj),]
head(res)

summary(res)

res_treatCD <- results(dds, name="treatment_D_vs_C", alpha=0.05)
res_treatCD <- res_treatCD[order(res_treatCD$padj),]
head(res_treatCD)
summary(res_treatCD)

plotMA(res_treatCD,ylim=c(-3,3))

vsd <- vst(dds, blind=FALSE)

data <- plotPCA(vsd,intgroup=c("climate","treatment","day"),returnData=TRUE)
percentVar <- round(100 * attr(data, "percentVar"))

data$treatment <- factor(data$treatment, levels=c("C","H","D"), labels = c("C","H","D"))
data$day <- factor(data$day, levels=c("0","5","10"), labels = c("0","5","10"))

ggplot(data, aes(PC1, PC2, color=day, shape=treatment)) +
  geom_point(size=4, alpha=0.85) +
  xlab(paste0("PC1: ",percentVar[1],"% variance")) +
  ylab(paste0("PC2: ",percentVar[2],"% variance")) +
  theme_minimal()

d <-plotCounts(dds, gene="MA_7017g0010", intgroup = (c("treatment","climate","day")), returnData=TRUE)
d

p <-ggplot(d, aes(x=treatment, y=count, color=day, shape=climate)) + 
  theme_minimal() + theme(text = element_text(size=20), panel.grid.major=element_line(colour="grey"))
p <- p + geom_point(position=position_jitter(w=0.3,h=0), size=3) +
  scale_x_discrete(limits=c("C","H","D"))
p

p <-ggplot(d, aes(x=treatment, y=count, shape=climate)) + 
  theme_minimal() + theme(text = element_text(size=20), panel.grid.major=element_line(colour="grey"))
p

library(pheatmap)
topgenes <- head(rownames(res_treatCD),20)
mat <- assay(vsd)[topgenes,]
mat <- mat - rowMeans(mat)
df <- as.data.frame(colData(dds)[,c("treatment","climate","day")])
pheatmap(mat, annotation_col=df)

```

#### Ways to explore data analysis
* Explore the various results for a given experimental design, see: resultsNames(dds)
* Set up your DESeq object with different experimental designs; e.g., design = ~ climate + treatment + climate:treatment
* Subset your data to exclude/include different factors, run different designs; select and subset are handy functions in R for          subsetting your countsMatrix or conds table, e.g., use only the day 10 data.

#### Analysis of DAY 10 samples

```
countsTable <- read.table("RS_cds2kb_countsMatrix.txt", header=TRUE, row.names=1)
head(countsTable)
dim(countsTable)
countsTableRound <- round(countsTable)
head(countsTableRound)
conds <- read.delim("RS_samples.txt", header=TRUE, stringsAsFactors = TRUE, row.names=1, colClasses=c('factor', 'factor', 'factor', 'factor'))
head(conds)
dim(conds)

grep("10", names(countsTableRound), value = TRUE)
day10countstable <- subset(countsTableRound, grep("10", names(countsTableRound), value = TRUE)) #doesn't work has to be logical

day10countstable <- countsTableRound %>% select(contains("10"))
dim(day10countstable)

conds10<- subset(conds, day=="10")
dim(conds10)
head(conds10)

colSums(day10countstable)
mean(colSums(day10countstable))
barplot(colSums(day10countstable), las=3, cex.names=0.5,names.arg = substring(colnames(day10countstable),1,13))
abline(h=mean(colSums(day10countstable)), col="blue", lwd =2)

rowSums(day10countstable)
mean(rowSums(day10countstable))
median(rowSums(day10countstable))

apply(day10countstable,2,mean)

dds <- DESeqDataSetFromMatrix(countData = day10countstable, colData = conds10, 
                              design = ~ climate + treatment + climate:treatment)
dim(dds)

dds <- dds[rowSums(counts(dds)) > 30]
dim(dds)

dds <- DESeq(dds)

resultsNames(dds)

res_treatCD <- results(dds, name="treatment_D_vs_C", alpha=0.05)
res_treatCD <- res_treatCD[order(res_treatCD$padj),]
head(res_treatCD)
summary(res_treatCD)
plotMA(res_treatCD,ylim=c(-3,3))

res_treatCH <- results(dds, name="treatment_H_vs_C", alpha = 0.05)
res_treatCH <- res_treatCH[order(res_treatCH$padj),]
head(res_treatCH)
summary(res_treatCH)
plotMA(res_treatCH,ylim=c(-3,3))

res_interClimTreat <- results(dds, name="climateHD.treatmentD", alpha=0.05)
res_interClimTreat <- res_interClimTreat[order(res_interClimTreat$padj),]
head(res_interClimTreat)
summary(res_interClimTreat)
plotMA(res_interClimTreat,ylim=c(-3,3))

res_interClimTreatH <- results(dds, name="climateHD.treatmentH", alpha=0.05)
res_interClimTreatH <- res_interClimTreatH[order(res_interClimTreatH$padj),]
head(res_interClimTreatH)
summary(res_interClimTreatH)
plotMA(res_interClimTreatH,ylim=c(-3,3))

res_ClimTreat <- results(dds, name="climate_HD_vs_CW", alpha=0.05)
res_ClimTreat <- res_ClimTreat[order(res_ClimTreat$padj),]
head(res_ClimTreat)
summary(res_ClimTreat)
plotMA(res_ClimTreat,ylim=c(-3,3))

res_Int <- results(dds, name="intercept", alpha = 0.05)
res_Int <- res_Int[order(res_Int$padj),]
head(res_Int)
summary(res_Int)
plotMA(res_Int,ylim=c(-3,3))

vsd <- vst(dds, blind=FALSE)

data <- plotPCA(vsd,intgroup=c("climate","treatment"),returnData=TRUE)
percentVar <- round(100 * attr(data, "percentVar"))

data$treatment <- factor(data$treatment, levels=c("C","H","D"), labels = c("Control","Hot","Dry+Hot"))
data$climate <- factor(data$climate, levels=c("HD","CW"), labels = c("Hot-Dry","Cold-Wet"))

ggplot(data, aes(PC1, PC2, color=climate, shape=treatment)) +
  geom_point(size=4, alpha=0.85) +
  xlab(paste0("PC1: ",percentVar[1],"% variance")) +
  ylab(paste0("PC2: ",percentVar[2],"% variance")) +
  theme_minimal()

d <-plotCounts(dds, gene="MA_133272g0010", intgroup = (c("treatment","climate")), returnData=TRUE)
d

p <-ggplot(d, aes(x=treatment, y=count, color=climate)) + 
  theme_minimal() + theme(text = element_text(size=20), panel.grid.major=element_line(colour="grey"))
p <- p + geom_point(position=position_jitter(w=0.3,h=0), size=3) +
  scale_x_discrete(limits=c("C","H","D"))
p

p <-ggplot(d, aes(x=treatment, y=count, shape=climate)) + 
  theme_minimal() + theme(text = element_text(size=20), panel.grid.major=element_line(colour="grey"))
p

library(pheatmap)
topgenes <- head(rownames(res_treatCD),20)
mat <- assay(vsd)[topgenes,]
mat <- mat - rowMeans(mat)
df <- as.data.frame(colData(dds)[,c("treatment","climate")])
pheatmap(mat, annotation_col=df)

```

------
<div id='id-section57'/>   

### Entry 57: 2020-03-19, Thursday.   



------
<div id='id-section58'/>   

### Entry 58: 2020-03-20, Friday.   



------
<div id='id-section59'/>   

### Entry 59: 2020-03-23, Monday.   



------
<div id='id-section60'/>   

### Entry 60: 2020-03-24, Tuesday.   



------
<div id='id-section61'/>   

### Entry 61: 2020-03-25, Wednesday.   

## Info-Update 

### Mechanism of Epigenetics 
* microRNA
* phosphorylation 
* methylation 
* histones 

### Changes to DNA methylation 
* Cytosine can go to Uracil after deanimation (can be corrected back) 
* Methalated cytosine can to Thymine ater demanimation (no corrected back) 

### Functional conesquneces of Methylation 
* Promotoers and enhancers 
  * High methylation = inhibtion of expression 
  * change chromatin structure 
  * inhibts transcription factors 
* Intergenic 
  * Silencing TE, etc 
* Gene Body 
  * gene body = intron and exon
  * high methylation inhibits random expression 
  * methylation allows for more stabalization of expression 
  * gene with low meth levels have higher to change response in environment
    * both hypotheses, not looked at in many organisms
* Literature review of different organisms of functional consequnces of methylation 

### Different Organism Methylation Phylogeny 
* Three different types of metyl: CG, CHG, CHH
* Plants have relativley high CHG, CG methylation that is heritable 
* Methylation patterns vary a lot between spp

### Patterns of Methylation with Genome
* Dip in promoter region methylation 
* Gene body also tend to be highly methylated in spp 
 
 ### Bisulfite Seq 
 * Frag genomic dna sample 
 * Conversion of C to T 
 * Any unmethalyted C will be converted to uracil 
 * Able to tell which were methylated and percentage 
 * What proportion of C at each site are methylated within many reads from one region of genome. 
 * Can be continuous trait. Non binary (some cells meythlated vs others that are not) 
 * Varies between organisms. Are also pooled from many indivisduals. 
 * Vertibrates usual on or off but can change for invetebrates. 
   * Early developmental pattern? Transmitted through cell divsions as opposed to acute response. 
   
 ## Liew et al 2020 Corals Paper Discussion 
 
 ### Patterns in Corals
 * Epigenetics important for long lived organims in particular 
 * Hertiable methyl patterns could be essential to adapt to change; ie resiliance in offspring in similar environments. 
 * Climate change implications; predictive patterns of change. 
 
### Figure 1 
* AD pop does experience bleaching, F population does not have bleaching. 
* Missing larva from one whole population. Two crosses within the AD population 
* RNA seq data would have helped a lot to demonstrate the point they are trying to make. 

### Figure 2 
* Does this really show that genetype does not correlate with methylation? 
* Reid and Melissa seems to think that there it does not. R=0.10 is good for Reid when compared to the entire genome. Signifcance for both genic and exonic regions. 
* Steve pointed out the demographic aspect of the varience described by the genic and exonic regions. 

### Figure 3 
* Does this show evidence for enviromental cause of metyhlation? 
* Verdict seems to be undecieded 
* Melissa says more work to be done. 

## Coding Session 

### Copepod Selection Experiment 

#### The organism 
* Sample organism: Arcartia tonsa
* Very abundance zoplankton > important ecological role. 
* Intresting b/c: large pops, large genetic diversity, and many different environments. High plasticity 
* Good model for how organisms could change in response to environment. Climate change. 
* Multigenerational experiments. 10000 of indviduals. 10 day genration time. 

#### Experimental Design 
* Treatments: Cntrl (AA), high co2 (AH), high temp (HA), temp+co2 (HH)
* Expect shifts in genrational and epigenetic responses. 
* Field collected and maintained for 3 generations before split up. 
* Samples collected at F0 and F25 
* 4 reps per treatment 

#### RRBS 
* "Reduced reference Bis sulfite sequencing"
* Reduced genome > Rad seq aproach >> restriction sites cuts (Mspl). Insenstive to meythlation. 
* End repair 
* adpater ligation 
* Bisulfite conversion of all unmeythl Cs to Ts 
* Small amount of DNA from E. coli as control to check downstream effects of how efficient bisulfite conversion was. 

#### Hypotheses and Questions 
1. Are there consistent patterns in each treatmnet group of methylations patterns? 
2. Are the stressfull treatements leading to responses that infer resiliance? 
3. What proportion of methyl sites are genic vs non genic? 
4. Is the combinded (TxCo2) treatment additive or mixed in response from the other two responses? 

#### Pipline 
1. Clean and visualize 
2. Align to reference genome
3. extract methylation calls 
4. Process filter calls 
5. Summary figures (PCA, etc) 
6. Test for differential methylation 

#### Cleaning 
* A lot more Ts due to Cs due to bisultife conversions on forward strand
* A lot of As in reverse strand due to the reverse complient of the Ts 
* Error increase at the end due to un methylated primers. 
* Primer contamination removed as well after trimming 
* Melissa says begging also strange but Reid did not cut off the start of the read. 

#### Aligning with Bismark 
* Aligning is different > some reads with almost no Cs and no Ts 
* Convert from G to C and T to A 
* Makes alignment harder to each forward and reverse section of genome

#### Running the Data 
* Make sure you `screen` before you run the script 
* My script is `AA_F00_1`
* Tutorial will be posted. Make sure its done by next Wednesday
* Could take 8-14 hours. So make sure to start it early. 








 
 

------
<div id='id-section62'/>   

### Entry 62: 2020-03-26, Thursday.   



------
<div id='id-section63'/>   

### Entry 63: 2020-03-27, Friday.   



------
<div id='id-section64'/>   

### Entry 64: 2020-03-30, Monday.   



------
<div id='id-section65'/>   

### Entry 65: 2020-03-31, Tuesday.   



------
<div id='id-section66'/>   

### Entry 66: 2020-04-01, Wednesday.   

#### Learning Objectives 
*Extract methylation calls and assess raw data
*Visualize genome-wide patterns
*Identify differential methylation at the SNP level


### Coding Session with Reed
```
install.packages("pheatmap")
BiocManager::install("methylKit")
install.packages("tidyverse")
library(pheatmap)
library(methylKit)
library(ggplot2)
library(tidyverse)

# first, we want to read in the raw methylation calls with methylkit

# set directory with absolute path (why is this necessary? I have no idea, but gz files wont work with relative paths)
dir <-"/Users/scottblack/Desktop/pbio381/Epigenetics/Epigenetics_data/Epigenetics_data"
# read in the sample ids
samples <-read.table("/Users/scottblack/Desktop/pbio381/Epigenetics/Epigenetics_data/Epigenetics_data/sample_id.txt",
                     header=FALSE)

# now point to coverage files
files <-file.path(dir, samples$V1)
all(file.exists(files))

# convert to list
file.list <-as.list(files)

# get the names only for naming our samples
nmlist <- as.list(gsub("_1_bismark_bt2_pe.bismark.cov.gz","",samples$V1))

# use methRead to read in the coverage files
myobj <- methRead(location= file.list,
                  sample.id =   nmlist,
                  assembly = "atonsa", # this is just a string. no actual database, creating another database to pool from
                  dbtype = "tabix",# database type
                  context = "CpG", # what to focus on 
                  resolution = "base", #data type = snp
                  mincov = 20, # req 20 reads per site (coverage)
                  treatment = #vector that indcates sample treatments that we have: AA F0, F25
                    c(0,0,0,0, # AA F0 (from sample names)
                      1,1,1,1, # AA F25
                      2,2,2,2, # AH F25
                      3,3,3,3, # HA F25
                      4,4,4,4),# HH F25
                  pipeline = "bismarkCoverage",
                  dbdir = "~/Desktop/pbio381/Epigenetics/Epigenetics_data/Epigenetics_data")
######
# visualize coverage and filter
######

# We can look at the coverage for individual samples with getCoverageStats()
getCoverageStats(myobj[[6]], plot=TRUE)

# and can plot all of our samples at once to compare.
# skip but could wrie a for loop for this

# filter samples by depth with filterByCoverage()
filtered.myobj <- filterByCoverage(myobj,
                                   lo.count=20, lo.perc = NULL,
                                   hi.count = NULL, hi.perc = 97.5,
                                   db.dir = "~/Desktop/pbio381/Epigenetics/Epigenetics_data/Epigenetics_data")

######
# merge samples
######

#Note! This takes a while and we're skipping it

# use unite() to merge all the samples. We will require sites to be present in each sample or else will drop it

meth <- unite(filtered.myobj, nc.cares=3, suffix="united",
              db.dir = "~/Desktop/pbio381/Epigenetics/Epigenetics_data/Epigenetics_data")


meth <- methylKit:::readMethylBaseDB(
  dbpath = "/Users/scottblack/Desktop/pbio381/Epigenetics/Epigenetics_data/Epigenetics_data/methylBase_united.txt.bgz",
  dbtype = "tabix",
  sample.id =   unlist(nmlist),
  assembly = "atonsa", # this is just a string. no actual database
  context = "CpG",
  resolution = "base",
  treatment = c(0,0,0,0,
                1,1,1,1,
                2,2,2,2,
                3,3,3,3,
                4,4,4,4),
  destrand = FALSE)
  

# percMethylation() calculates the percent methylation for each site and sample
pm <-percMethylation(meth)


#plot methylation histograms
ggplot(gather(as.data.frame(pm)), aes(value)) + 
  geom_histogram(bins = 10, color="black", fill="grey") + 
  facet_wrap(~key)

# calculate and plot mean methylation
sp.means  <-colMeans(pm)

p.df <- data.frame(sample=names(sp.means),
                   group = substr(names(sp.means), 1,6),
                   methylation = sp.means)

ggplot(p.df, aes(x=group, y=methylation, color=group)) + 
  stat_summary(color="black") + geom_jitter(width=0.1, size=3) + 
  xlab(paste0("Treatment")) +
  ylab(paste0("% Methylation")) 

# of sites: 
dim(pm)

# sample clustering
clusterSamples(meth, dist = "correlation", method = "ward.D", plot = TRUE)

# PCA, REID WILL POST

PCASamples(meth, screeplot=TRUE)
PCASamples(meth, screeplot=FALSE)

# subset with reorganize()

meth_sub <- reorganize(meth,
                       sample.ids =c("AA_F25_1","AA_F25_2","AA_F25_3", "AA_F25_4",
                                     "HH_F25_1","HH_F25_2","HH_F25_3","HH_F25_4"),
                       treatment = c(0,0,0,0,1,1,1,1),
                       save.db=FALSE)



# calculate differential methylation

myDiff <- calculateDiffMeth(meth_sub,
                            overdispersion = "MN",
                            mc.cores = 1,
                            suffix = "AA_HH",
                            adjust = "qvalue",
                            test = "Chisq")

# get all differentially methylated bases
myDiff <-getMethylDiff(myDiff, qvalue = 0.05, difference=10)

# we can visualize the changes in methylation frequencies quickly.
hist(getData(myDiff)$meth.diff)


# get hyper methylated bases
hyper=getMethylDiff(myDiff,difference=10,qvalue=0.05,type="hyper")

# get hypo methylated bases
hypo=getMethylDiff(myDiff,difference=10,qvalue=0.05,type="hypo")

#heatmaps first

# get percent methylation matrix
pm <- percMethylation(meth_sub)
# make a dataframe with snp id's, methylation, etc.
sig.in <- as.numeric(row.names(myDiff))
pm.sig <- pm[sig.in,]

# add snp, chr, start, stop
din <- getData(myDiff)[,1:3]
df.out <- cbind(paste(getData(myDiff)$chr, getData(myDiff)$start, sep=":"), din, pm.sig)
colnames(df.out) <- c("snp", colnames(din), colnames(df.out[5:ncol(df.out)]))
df.out <- (cbind(df.out,getData(myDiff)[,5:7]))



####
# heatmap
####

my_heatmap <- pheatmap(pm.sig,
                       show_rownames = FALSE)

topgenes <- head(rownames(pm.sig),20)

# we can also normalize 

my_heatmap <- pheatmap(pm.sig,
                       show_rownames = FALSE)

ctrmean <- rowMeans(pm.sig[,1:4])

h.norm <- (pm.sig-ctrmean)

my_heatmap <- pheatmap(h.norm,
                       show_rownames = FALSE)

#####
#let's look at methylation of specific snps
####
df.out
df.plot <- df.out[,c(1,5:12)] %>% pivot_longer(-snp, values_to = "methylation")
df.plot$group <- substr(df.plot$name,1,2)
head(df.plot)

# convert data frame to long form
df.plot %>% filter(snp=="LS043685.1:10221") %>% 
  ggplot(., aes(x=group, y=methylation, color=group, fill=group)) +
  stat_summary(fun.data = "mean_se", size = 2) +
  geom_jitter(width = 0.1, size=3, pch=21, color="black")


## write bed file for intersection with genome annotation
write.table(file = "/Users/scottblack/Desktop/pbio381/Epigenetics/Epigenetics_data/Epigenetics_data/diffmeth2.bed",
            data.frame(chr= df.out$chr, start = df.out$start, end = df.out$end),
            row.names=FALSE, col.names=FALSE, quote=FALSE, sep="\t")
```

------
<div id='id-section67'/>   

### Entry 67: 2020-04-02, Thursday.   



------
<div id='id-section68'/>   

### Entry 68: 2020-04-03, Friday.   



------
<div id='id-section69'/>   

### Entry 69: 2020-04-06, Monday.   

### Updates 
* Monday, 4/6 - Intro/background on group projects; start sharing ideas, groups coalesce
* Wed, 4/8 - open lab related to HW #3 - half time; second half - groups meet to develop proposal
* Friday, 4/10 - HW #3 due
* Monday, 4/13 - Groups present on proposed projects (brief: ~5 slides to present study system, questions/hypotheses, approach, and anticipated results/outcomes)
* Updated code on general area of Slack

### Review of Sections 
* Sections:
   1. Populations genomics of diversity within red spruce range edge populations.
   * Genomic DNA, diversity metrics (of sequence) 
   2. Transcriptomics of red spruce gene expression to heat and drought stressors. 
   * expression diversity, expression data. 
   3. Epigenomics: Epigenmoics of copepod methylation during experimental evolution under temp and CO2 stress. 

 * Mapping
 * Assessing quality, cleaning 
 * Statistics
 * Scratching surface with of what data shows. 
 
 ### Group Projects 
 * Goals: 
  1. Give opportunity to dig deepers in to area of intrests. 
  2. Pose new questions and learn new analyses/tools. 
  3. Prob analysis of assumptions or choices made. 
  4. Inegrating across existing or new datasets 
   * Genomic seq
   * Transcriptomic seq
   * Epigenmoic seq
     * Functional annotation data
     * Environmental data 
     * Phenotype data 

* Expectation: 
  * Work in teams (2-5 students) 
  * Develop questions
  * Choose to work with one or more class datasets 
  * Present the intial plan for feedback (week from today) 
  * Devirse and implement analysis strategy (divide and conquer) 
  * 3 days of open lab 
  * Present final results at the end 
  * write and final paper (individual or group co-authored) 

* Agenda for today: 
  * Refresh on each of the class datasets
   * Opportunity for additonal student datasets
  * Start brainstorm 
  * Coalesce into groups based on shared ideas/questions/tools
  * Small group meetings to start planning

* Brainstorming document: 
 * https://uvmoffice.sharepoint.com/sites/EcologicalGenomics/_layouts/15/Doc.aspx?sourcedoc={5aba0c0e-6810-4131-9768-4af55efec99c}&action=view&wd=target%28_Collaboration%20Space%2FGroup%20Projects.one%7C5c40bf9e-4e7c-461d-ac09-66adeace8753%2FBrainstorming%20for%20Group%20Projects%7C3661696a-383e-4875-82be-c0cf9118e0a0%2F%29
 
 
------
<div id='id-section70'/>   

### Entry 70: 2020-04-07, Tuesday.   



------
<div id='id-section71'/>   

### Entry 71: 2020-04-08, Wednesday.   



------
<div id='id-section72'/>   

### Entry 72: 2020-04-09, Thursday.   



------
<div id='id-section73'/>   

### Entry 73: 2020-04-10, Friday.   



------
<div id='id-section74'/>   

### Entry 74: 2020-04-13, Monday.   



------
<div id='id-section75'/>   

### Entry 75: 2020-04-14, Tuesday.   



------
<div id='id-section76'/>   

### Entry 76: 2020-04-15, Wednesday.   



------
<div id='id-section77'/>   

### Entry 77: 2020-04-16, Thursday.   



------
<div id='id-section78'/>   

### Entry 78: 2020-04-17, Friday.   



------
<div id='id-section79'/>   

### Entry 79: 2020-04-20, Monday.   



------
<div id='id-section80'/>   

### Entry 80: 2020-04-21, Tuesday.   



------
<div id='id-section81'/>   

### Entry 81: 2020-04-22, Wednesday.   

## Glaciation Project Scripts 

### PWD:
```
/data/project_data/GroupProjects/Glaciation
```

### Extract Bam Files from each region to bam.list files 
```
#use grep on the metadata file that contains which region each individual belongs to. 
grep -w "E" RS_Exome_metadata.txt | cut -f3 >/pathtoyourgroupfolder/EdgeInds.txt

#do that in turn for each of the other regions want to pull out…

# cd to the all/ folder with all the bams and do:

/data/project_data/RS_Exome/mapping/BWA/all
ls *bam* | grep -f /pathtogroupfolder/EdgeInds.txt >/pathtogroupfolder/EdgeBams.bamlist

#do that in turn for each of the other regions want to pull out…
```
### ANGSD Scripts for each region (CORE Example)

```
#!/ein/bash

myrepo="/data/project_data/GroupProjects/Glaciation"

#mkdir ${myrepo}/myresults/ANGSD

output="${myrepo}/REGIONS/CORE"

POP="CORE"

#ls /data/project_data/RS_ExomeSeq/mapping/BWA/${mypop}_*sorted.rm*.bam >${output}/${mypop}_bam.list

REF="/data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa"

ANGSD -b ${output}/${POP}_bam.list \
-ref ${REF} \
-anc ${REF} \
-out ${output}/${POP} \
-nThreads 4 \
-remove_bads 1 \
-uniqueOnly 1 \
-C 50 \
-baq 1 \
-minMapQ 20 \
-minQ 20 \
-minInd 2 \
-setMinDepthInd 1 \
-setMaxDepthInd 17 \
-skipTriallelic 1 \
-GL 1 \
-doHWE 1 \
-doCounts 1 \
-doMaf 1 \
-doSaf 1 \
-doMajorMinor 1 \
-doGlf 1

#do this in turn for each of the other regions
```
###


<div id='id-section82'/>   

### Entry 82: 2020-04-23, Thursday.   



------
<div id='id-section83'/>   

### Entry 83: 2020-04-24, Friday.   

### Finding loci intersection among regions
```
#!/bin/bash

myrepo="/data/project_data/GroupProjects/Glaciation"

output="${myrepo}/REGIONS"

POP1="CORE"
POP2="EDGE"
POP3="MARGIN"

REF="/data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa"

# Finding loci intersection among regions
#Transform .gz files into site text files
zcat ${output}/${POP1}/CORE.mafs.gz | awk 'BEGIN { OFS = ":" }{ print $1,$2 }' | sed '1d' | sort > ${output}/${POP1}/CORE_sites.txt
zcat ${output}/${POP2}/EDGE.mafs.gz | awk 'BEGIN { OFS = ":" }{ print $1,$2 }' | sed '1d' | sort > ${output}/${POP2}/EDGE_sites.txt
zcat ${output}/${POP3}/MARGIN.mafs.gz | awk 'BEGIN { OFS = ":" }{ print $1,$2 }' | sed '1d' | sort > ${output}/${POP3}/MARGIN_sites.txt

#Compare region text files and unite into one main sites text file
comm -12 ${output}/${POP1}/CORE_sites.txt ${output}/${POP2}/EDGE_sites.txt > ${output}/CORE_EDGE_sites.txt
comm -12 ${output}/CORE_EDGE_sites.txt ${output}/${POP3}/MARGIN_sites.txt > ${output}/CORE_EDGE_MARGIN_sites.txt

#Sort, cut, and index into intersecting sites text file
sed 's/:/\t/' ${output}/CORE_EDGE_MARGIN_sites.txt | sort -b -k1,1 > ${output}/intersect.txt
cut -f1 ${output}/intersect.txt | uniq | sort > ${output}/intersect.chrs
ANGSD sites index ${output}/intersect.txt

# Number of total sites with monomorphic and plolymorphic sites
cat ${output}/intersect.txt | wc -l # 42,328,740 sites

```


------
<div id='id-section84'/>   

### Entry 84: 2020-04-27, Monday.   



------
<div id='id-section85'/>   

### Entry 85: 2020-04-28, Tuesday.   



------
<div id='id-section86'/>   

### Entry 86: 2020-04-29, Wednesday.   



------
<div id='id-section87'/>   

### Entry 87: 2020-04-30, Thursday.   



------
<div id='id-section88'/>   

### Entry 88: 2020-05-01, Friday.   



------
<div id='id-section89'/>   

### Entry 89: 2020-05-04, Monday.   



------
<div id='id-section90'/>   

### Entry 90: 2020-05-05, Tuesday.   



------
<div id='id-section91'/>   

### Entry 91: 2020-05-06, Wednesday.   



------
<div id='id-section92'/>   

### Entry 92: 2020-05-07, Thursday.   



------
<div id='id-section93'/>   

### Entry 93: 2020-05-08, Friday.   



------
<div id='id-section94'/>   

### Entry 94: 2020-05-11, Monday.   



------
<div id='id-section95'/>   

### Entry 95: 2020-05-12, Tuesday.   



------
<div id='id-section96'/>   

### Entry 96: 2020-05-13, Wednesday.   



------
<div id='id-section97'/>   

### Entry 97: 2020-05-14, Thursday.   



------
<div id='id-section98'/>   

### Entry 98: 2020-05-15, Friday.   



------
<div id='id-section99'/>   

### Entry 99: 2020-05-18, Monday.   



------
<div id='id-section100'/>   

### Entry 100: 2020-05-19, Tuesday.   



------
<div id='id-section101'/>   

### Entry 101: 2020-05-20, Wednesday.   



------
<div id='id-section102'/>   

### Entry 102: 2020-05-21, Thursday.   



------
<div id='id-section103'/>   

### Entry 103: 2020-05-22, Friday.   



------
<div id='id-section104'/>   

### Entry 104: 2020-05-25, Monday.   



------
<div id='id-section105'/>   

### Entry 105: 2020-05-26, Tuesday.   



------
<div id='id-section106'/>   

### Entry 106: 2020-05-27, Wednesday.   



------
<div id='id-section107'/>   

### Entry 107: 2020-05-28, Thursday.   



------
<div id='id-section108'/>   

### Entry 108: 2020-05-29, Friday.   



------
<div id='id-section109'/>   

### Entry 109: 2020-06-01, Monday.   



------
<div id='id-section110'/>   

### Entry 110: 2020-06-02, Tuesday.   



------
<div id='id-section111'/>   

### Entry 111: 2020-06-03, Wednesday.   



------
<div id='id-section112'/>   

### Entry 112: 2020-06-04, Thursday.   



------
<div id='id-section113'/>   

### Entry 113: 2020-06-05, Friday.   



------
<div id='id-section114'/>   

### Entry 114: 2020-06-08, Monday.   



------
<div id='id-section115'/>   

### Entry 115: 2020-06-09, Tuesday.   



------
<div id='id-section116'/>   

### Entry 116: 2020-06-10, Wednesday.   



------
<div id='id-section117'/>   

### Entry 117: 2020-06-11, Thursday.   



------
<div id='id-section118'/>   

### Entry 118: 2020-06-12, Friday.   



------
<div id='id-section119'/>   

### Entry 119: 2020-06-15, Monday.   



------
<div id='id-section120'/>   

### Entry 120: 2020-06-16, Tuesday.   



------
<div id='id-section121'/>   

### Entry 121: 2020-06-17, Wednesday.   



------
<div id='id-section122'/>   

### Entry 122: 2020-06-18, Thursday.   



------
<div id='id-section123'/>   

### Entry 123: 2020-06-19, Friday.   



------
<div id='id-section124'/>   

### Entry 124: 2020-06-22, Monday.   



------
<div id='id-section125'/>   

### Entry 125: 2020-06-23, Tuesday.   



------
<div id='id-section126'/>   

### Entry 126: 2020-06-24, Wednesday.   



------
<div id='id-section127'/>   

### Entry 127: 2020-06-25, Thursday.   



------
<div id='id-section128'/>   

### Entry 128: 2020-06-26, Friday.   



------
<div id='id-section129'/>   

### Entry 129: 2020-06-29, Monday.   



------
<div id='id-section130'/>   

### Entry 130: 2020-06-30, Tuesday.   



------
<div id='id-section131'/>   

### Entry 131: 2020-07-01, Wednesday.   



------
<div id='id-section132'/>   

### Entry 132: 2020-07-02, Thursday.   



------
<div id='id-section133'/>   

### Entry 133: 2020-07-03, Friday.   



------
<div id='id-section134'/>   

### Entry 134: 2020-07-06, Monday.   



------
<div id='id-section135'/>   

### Entry 135: 2020-07-07, Tuesday.   



------
<div id='id-section136'/>   

### Entry 136: 2020-07-08, Wednesday.   



------
<div id='id-section137'/>   

### Entry 137: 2020-07-09, Thursday.   



------
<div id='id-section138'/>   

### Entry 138: 2020-07-10, Friday.   



------
<div id='id-section139'/>   

### Entry 139: 2020-07-13, Monday.   



------
<div id='id-section140'/>   

### Entry 140: 2020-07-14, Tuesday.   



------
<div id='id-section141'/>   

### Entry 141: 2020-07-15, Wednesday.   



------
<div id='id-section142'/>   

### Entry 142: 2020-07-16, Thursday.   



------
<div id='id-section143'/>   

### Entry 143: 2020-07-17, Friday.   



------
<div id='id-section144'/>   

### Entry 144: 2020-07-20, Monday.   



------
<div id='id-section145'/>   

### Entry 145: 2020-07-21, Tuesday.   



------
<div id='id-section146'/>   

### Entry 146: 2020-07-22, Wednesday.   



------
<div id='id-section147'/>   

### Entry 147: 2020-07-23, Thursday.   



------
<div id='id-section148'/>   

### Entry 148: 2020-07-24, Friday.   



------
<div id='id-section149'/>   

### Entry 149: 2020-07-27, Monday.   



------
<div id='id-section150'/>   

### Entry 150: 2020-07-28, Tuesday.   



------
<div id='id-section151'/>   

### Entry 151: 2020-07-29, Wednesday.   



------
<div id='id-section152'/>   

### Entry 152: 2020-07-30, Thursday.   



------
<div id='id-section153'/>   

### Entry 153: 2020-07-31, Friday.   



------
<div id='id-section154'/>   

### Entry 154: 2020-08-03, Monday.   



------
<div id='id-section155'/>   

### Entry 155: 2020-08-04, Tuesday.   



------
<div id='id-section156'/>   

### Entry 156: 2020-08-05, Wednesday.   



------
<div id='id-section157'/>   

### Entry 157: 2020-08-06, Thursday.   



------
<div id='id-section158'/>   

### Entry 158: 2020-08-07, Friday.   



------
<div id='id-section159'/>   

### Entry 159: 2020-08-10, Monday.   



------
<div id='id-section160'/>   

### Entry 160: 2020-08-11, Tuesday.   



------
<div id='id-section161'/>   

### Entry 161: 2020-08-12, Wednesday.   



------
<div id='id-section162'/>   

### Entry 162: 2020-08-13, Thursday.   



------
<div id='id-section163'/>   

### Entry 163: 2020-08-14, Friday.   



------
<div id='id-section164'/>   

### Entry 164: 2020-08-17, Monday.   



------
<div id='id-section165'/>   

### Entry 165: 2020-08-18, Tuesday.   



------
<div id='id-section166'/>   

### Entry 166: 2020-08-19, Wednesday.   



------
<div id='id-section167'/>   

### Entry 167: 2020-08-20, Thursday.   



------
<div id='id-section168'/>   

### Entry 168: 2020-08-21, Friday.   



------
<div id='id-section169'/>   

### Entry 169: 2020-08-24, Monday.   



------
<div id='id-section170'/>   

### Entry 170: 2020-08-25, Tuesday.   



------
<div id='id-section171'/>   

### Entry 171: 2020-08-26, Wednesday.   



------
<div id='id-section172'/>   

### Entry 172: 2020-08-27, Thursday.   



------
<div id='id-section173'/>   

### Entry 173: 2020-08-28, Friday.   



------
<div id='id-section174'/>   

### Entry 174: 2020-08-31, Monday.   



------
<div id='id-section175'/>   

### Entry 175: 2020-09-01, Tuesday.   



------
<div id='id-section176'/>   

### Entry 176: 2020-09-02, Wednesday.   



------
<div id='id-section177'/>   

### Entry 177: 2020-09-03, Thursday.   



------
<div id='id-section178'/>   

### Entry 178: 2020-09-04, Friday.   



------
<div id='id-section179'/>   

### Entry 179: 2020-09-07, Monday.   



------
<div id='id-section180'/>   

### Entry 180: 2020-09-08, Tuesday.   



------
<div id='id-section181'/>   

### Entry 181: 2020-09-09, Wednesday.   



------
<div id='id-section182'/>   

### Entry 182: 2020-09-10, Thursday.   



------
<div id='id-section183'/>   

### Entry 183: 2020-09-11, Friday.   



------
<div id='id-section184'/>   

### Entry 184: 2020-09-14, Monday.   



------
<div id='id-section185'/>   

### Entry 185: 2020-09-15, Tuesday.   



------
<div id='id-section186'/>   

### Entry 186: 2020-09-16, Wednesday.   



------
<div id='id-section187'/>   

### Entry 187: 2020-09-17, Thursday.   



------
<div id='id-section188'/>   

### Entry 188: 2020-09-18, Friday.   



------
<div id='id-section189'/>   

### Entry 189: 2020-09-21, Monday.   



------
<div id='id-section190'/>   

### Entry 190: 2020-09-22, Tuesday.   



------
<div id='id-section191'/>   

### Entry 191: 2020-09-23, Wednesday.   



------
<div id='id-section192'/>   

### Entry 192: 2020-09-24, Thursday.   



------
<div id='id-section193'/>   

### Entry 193: 2020-09-25, Friday.   



------
<div id='id-section194'/>   

### Entry 194: 2020-09-28, Monday.   



------
<div id='id-section195'/>   

### Entry 195: 2020-09-29, Tuesday.   



------
<div id='id-section196'/>   

### Entry 196: 2020-09-30, Wednesday.   



------
<div id='id-section197'/>   

### Entry 197: 2020-10-01, Thursday.   



------
<div id='id-section198'/>   

### Entry 198: 2020-10-02, Friday.   



------
<div id='id-section199'/>   

### Entry 199: 2020-10-05, Monday.   



------
<div id='id-section200'/>   

### Entry 200: 2020-10-06, Tuesday.   



------
<div id='id-section201'/>   

### Entry 201: 2020-10-07, Wednesday.   



------
<div id='id-section202'/>   

### Entry 202: 2020-10-08, Thursday.   



------
<div id='id-section203'/>   

### Entry 203: 2020-10-09, Friday.   



------
<div id='id-section204'/>   

### Entry 204: 2020-10-12, Monday.   



------
<div id='id-section205'/>   

### Entry 205: 2020-10-13, Tuesday.   



------
<div id='id-section206'/>   

### Entry 206: 2020-10-14, Wednesday.   



------
<div id='id-section207'/>   

### Entry 207: 2020-10-15, Thursday.   



------
<div id='id-section208'/>   

### Entry 208: 2020-10-16, Friday.   



------
<div id='id-section209'/>   

### Entry 209: 2020-10-19, Monday.   



------
<div id='id-section210'/>   

### Entry 210: 2020-10-20, Tuesday.   



------
<div id='id-section211'/>   

### Entry 211: 2020-10-21, Wednesday.   



------
<div id='id-section212'/>   

### Entry 212: 2020-10-22, Thursday.   



------
<div id='id-section213'/>   

### Entry 213: 2020-10-23, Friday.   



------
<div id='id-section214'/>   

### Entry 214: 2020-10-26, Monday.   



------
<div id='id-section215'/>   

### Entry 215: 2020-10-27, Tuesday.   



------
<div id='id-section216'/>   

### Entry 216: 2020-10-28, Wednesday.   



------
<div id='id-section217'/>   

### Entry 217: 2020-10-29, Thursday.   



------
<div id='id-section218'/>   

### Entry 218: 2020-10-30, Friday.   



------
<div id='id-section219'/>   

### Entry 219: 2020-11-02, Monday.   



------
<div id='id-section220'/>   

### Entry 220: 2020-11-03, Tuesday.   



------
<div id='id-section221'/>   

### Entry 221: 2020-11-04, Wednesday.   



------
<div id='id-section222'/>   

### Entry 222: 2020-11-05, Thursday.   



------
<div id='id-section223'/>   

### Entry 223: 2020-11-06, Friday.   



------
<div id='id-section224'/>   

### Entry 224: 2020-11-09, Monday.   



------
<div id='id-section225'/>   

### Entry 225: 2020-11-10, Tuesday.   



------
<div id='id-section226'/>   

### Entry 226: 2020-11-11, Wednesday.   



------
<div id='id-section227'/>   

### Entry 227: 2020-11-12, Thursday.   



------
<div id='id-section228'/>   

### Entry 228: 2020-11-13, Friday.   



------
<div id='id-section229'/>   

### Entry 229: 2020-11-16, Monday.   



------
<div id='id-section230'/>   

### Entry 230: 2020-11-17, Tuesday.   



------
<div id='id-section231'/>   

### Entry 231: 2020-11-18, Wednesday.   



------
<div id='id-section232'/>   

### Entry 232: 2020-11-19, Thursday.   



------
<div id='id-section233'/>   

### Entry 233: 2020-11-20, Friday.   



------
<div id='id-section234'/>   

### Entry 234: 2020-11-23, Monday.   



------
<div id='id-section235'/>   

### Entry 235: 2020-11-24, Tuesday.   



------
<div id='id-section236'/>   

### Entry 236: 2020-11-25, Wednesday.   



------
<div id='id-section237'/>   

### Entry 237: 2020-11-26, Thursday.   



------
<div id='id-section238'/>   

### Entry 238: 2020-11-27, Friday.   



------
<div id='id-section239'/>   

### Entry 239: 2020-11-30, Monday.   



------
<div id='id-section240'/>   

### Entry 240: 2020-12-01, Tuesday.   



------
<div id='id-section241'/>   

### Entry 241: 2020-12-02, Wednesday.   



------
<div id='id-section242'/>   

### Entry 242: 2020-12-03, Thursday.   



------
<div id='id-section243'/>   

### Entry 243: 2020-12-04, Friday.   



------
<div id='id-section244'/>   

### Entry 244: 2020-12-07, Monday.   



------
<div id='id-section245'/>   

### Entry 245: 2020-12-08, Tuesday.   



------
<div id='id-section246'/>   

### Entry 246: 2020-12-09, Wednesday.   



------
<div id='id-section247'/>   

### Entry 247: 2020-12-10, Thursday.   



------
<div id='id-section248'/>   

### Entry 248: 2020-12-11, Friday.   



------
<div id='id-section249'/>   

### Entry 249: 2020-12-14, Monday.   



------
<div id='id-section250'/>   

### Entry 250: 2020-12-15, Tuesday.   



------
<div id='id-section251'/>   

### Entry 251: 2020-12-16, Wednesday.   



------
<div id='id-section252'/>   

### Entry 252: 2020-12-17, Thursday.   



------
<div id='id-section253'/>   

### Entry 253: 2020-12-18, Friday.   



------
<div id='id-section254'/>   

### Entry 254: 2020-12-21, Monday.   



------
<div id='id-section255'/>   

### Entry 255: 2020-12-22, Tuesday.   



------
<div id='id-section256'/>   

### Entry 256: 2020-12-23, Wednesday.   



------
<div id='id-section257'/>   

### Entry 257: 2020-12-24, Thursday.   



------
<div id='id-section258'/>   

### Entry 258: 2020-12-25, Friday.   



------
<div id='id-section259'/>   

### Entry 259: 2020-12-28, Monday.   



------
<div id='id-section260'/>   

### Entry 260: 2020-12-29, Tuesday.   



------
<div id='id-section261'/>   

### Entry 261: 2020-12-30, Wednesday.   



------
<div id='id-section262'/>   

### Entry 262: 2020-12-31, Thursday.   
